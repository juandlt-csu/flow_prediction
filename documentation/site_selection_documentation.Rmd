---
title: "Site Selection"
output: 
  html_document:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = FALSE, include = FALSE, warning = FALSE}
# To install StreamCatTools:
# library(remotes)
# install_github("USEPA/StreamCatTools", build_vignettes=FALSE)

# To install climateR:
# library(remotes)
# remotes::install_github("mikejohnson51/AOI") # suggested!
# remotes::install_github("mikejohnson51/climateR")

# To install ggsankey:
# library(remotes)
# remotes::install_github("davidsjoberg/ggsankey")

packages <- c('tidyverse',
              'sf',
              'terra',
              'elevatr', 
              'dataRetrieval',
              'nhdplusTools',
              'StreamCatTools',
              'tmap',
              'climateR',
              'data.table',
              'mapview',
              'here',
              'furrr',
              'nngeo',
              'retry',
              'units',
              'FedData',
              'knitr',
              'DT',
              'ggsankey')

# this package loader avoids unloading and reloading packages 
package_loader <- function(x) {
  if (!requireNamespace(x, quietly = TRUE)) {
    install.packages(x)
  }
  require(x, character.only = TRUE)
}

lapply(packages, package_loader)
```

```{r, echo=FALSE}
# Read in all of the datasets for the sankey diagram and later evaluation

## Initial gage pull via USGS
nwis_sites <- read_rds(here("data", "nwis_sites_pull.rds")) %>% 
  arrange(STUSPS) 

## Initial filter of gages via USGS
data_filter_nwis_sites <- read_rds(here("data", "initial_filter_nwis_sites.rds")) %>% 
  arrange(STUSPS)

## Initial pull and filter of CDWR sites
cdwr_sites <- read_rds(here("data", "cdwr_sites.rds"))

## USGS and CDWR sites combined
gage_sites <- read_rds(here("data", "usgs_cdwr_sites.rds")) %>% 
  arrange(STUSPS)

## Delineation area filter step
watershed_polygons <- read_rds(here("data", "watershed_polygons.rds"))

## Streamcat variable filter step
streamcat_watersheds <- read_rds(here("data", "streamcat_watersheds.rds"))

## Watershed div step
watersheds_div <- read_rds(here('data', 'watersheds_div.rds'))

## Final filter step
filtered_transbasin_watersheds <- watersheds_div %>% 
  filter(transbasin == "NATURAL")
```

# Data criteria for site selection overview

The site selection criteria is as automated as possible using data available via
USGS National Water Information System (NWIS), Colorado Division of Water Resources (CDWR), National Hydrography Dataset (NHD/NHDPlus), EPA's StreamCat database, and the National Land Cover Database (NLCD). The site selection criteria aims
to replicate the same site standards that were used in Eurich et al. to obtain
sites with minimal human disturbance. 

We searched for sites across Colorado, Wyoming, Utah, and Kansas that can serve 
as reference sites for studying natural hydrological processes. We used a systematic, 
multi-step filtering approach to ensure the selected watersheds have long-term, 
high quality discharge records while minimizing human impacts that could alter 
natural flow patterns. 

<span style="color: red;">**NOTICE: This is preliminary data, and is not the final dataset.**</span>

## Site selection methodology overview:

1. _Initial gage identification:_ We began by identifying all USGS stream gages 
measuring discharge in our four states of interest, focusing on watersheds 
smaller than 1500 km² to maintain hydrological consistency.
2. _Data quality and longevity:_ We required continuous records spanning at least 
30 years (starting by 1980 and continuing through at least 2010) to capture 
long-term hydrological data.
3. _Data source expansion:_ We supplemented USGS gages with Colorado Division of 
Water Resources (CDWR) gages meeting the same criteria.
4. Watershed delineation via NHDPlus network topology: We used multiple methods to verify each 
gage's position within the stream network and delineated watershed boundaries 
using NHDPlus topology.
5. _Human modification filtering:_ We applied filters to exclude watersheds with 
significant anthropogenic alterations:
  - Removed watersheds with more than 10% urban development
  - Removed watersheds with excessive dam storage density (>100,000 ML/km²)
  - Removed watersheds with transbasin diversions (artificial water transfers 
  crossing watershed boundaries)

This filtering approach reduced our initial dataset from `r nrow(nwis_sites) + nrow(cdwr_sites)` 
potential sites to just `r nrow(filtered_transbasin_watersheds)` high-quality 
reference sites. Each remaining watershed has minimal human modification, contains 
reliable long-term discharge records, and maintains natural hydrological boundaries

### Diagram of filtering process
```{r, echo = FALSE}
# Get row data information
nrows_step_1 <- nrow(nwis_sites)
nrows_step_2 <- nrow(data_filter_nwis_sites)
nrows_step_3 <- nrow(gage_sites)
nrows_step_4 <- nrow(watershed_polygons)
nrows_step_5 <- nrow(streamcat_watersheds)
nrows_step_6 <- nrow(filtered_transbasin_watersheds)

# Create filter stages data
# Create filter stages data
# Create filter stages data
filter_stages <- tribble(
  ~Step,     ~`Initial \nNWIS Sites`,      ~`Time Series \nFilter`,        ~`CDWR \nAdditions`,         ~`Watershed \nDelineation`,     ~`Urban/Dam \nFilter`,          ~`Transbasin \nExclusion`,
  "Step 1",  nrows_step_1,              nrows_step_2,                 nrows_step_3,                nrows_step_4,                 nrows_step_5,                 nrows_step_6,
  "Step 2",  nrows_step_1,              nrows_step_2,                 nrows_step_3,                nrows_step_4,                 nrows_step_5,                 nrows_step_1 - nrows_step_6,
  "Step 3",  nrows_step_1,              nrows_step_2,                 nrows_step_3,                nrows_step_4,                 nrows_step_1 - nrows_step_5,  nrows_step_1 - nrows_step_6,
  "Step 4",  nrows_step_1,              nrows_step_2,                 nrows_step_3,                nrows_step_1 - nrows_step_4,  nrows_step_1 - nrows_step_5,  nrows_step_1 - nrows_step_6,
  "Step 5",  nrows_step_1,              nrows_step_2,                 nrows_step_1 - nrows_step_3, nrows_step_1 - nrows_step_4,  nrows_step_1 - nrows_step_5,  nrows_step_1 - nrows_step_6,
  "Step 6",  nrows_step_1,              nrows_step_1 - nrows_step_2,   nrows_step_1 - nrows_step_3, nrows_step_1 - nrows_step_4,  nrows_step_1 - nrows_step_5,  nrows_step_1 - nrows_step_6
) %>% 
  make_long( `Initial \nNWIS Sites`, `Time Series \nFilter`, `CDWR \nAdditions`, `Watershed \nDelineation`, `Urban/Dam \nFilter`, `Transbasin \nExclusion`)

# Create Sankey plot ----
create_pattern <- function(n_rows) {
  pattern <- character(n_rows)
  cycles_needed <- ceiling(n_rows / 6)
  
  for (cycle in 1:cycles_needed) {
    downs <- 6 - cycle + 1
    ups <- 6 - downs
    
    if (downs < 1) downs <- 1
    if (ups < 1) ups <- 1
    
    cycle_pattern <- c(rep("down", downs), rep("up", ups))
    
    start_idx <- (cycle - 1) * 6 + 1
    end_idx <- min(start_idx + 5, n_rows)
    
    pattern[start_idx:end_idx] <- cycle_pattern[1:(end_idx - start_idx + 1)]
  }
  
  return(pattern)
}

# Apply to your dataframe
filter_stages$direction <- create_pattern(nrow(filter_stages))

# For example, make some flows wider than others
filter_stages <- filter_stages %>%
  mutate(weight = case_when(
    x == "Initial NWIS Sites" ~ 2,
    x == "Time Series Filter" ~ 3,
    x == "CDWR Additions" ~ 4,
    TRUE ~ 5
  ))

# Sankey plot
sankey_plot <- ggplot(filter_stages, aes(x = x, 
                                         next_x = next_x,
                                         node = node,
                                         next_node = next_node,
                                         fill = direction,
                                         label = node,
                                         weight = weight)) +
  geom_sankey(flow.alpha = 0.4,
              node.color = "gray30",
              show.legend = FALSE) +
  scale_fill_manual(values = c("down" = "#2ECC71", "up" = "#E74C3C")) +
  geom_sankey_label(size = 3, 
                    color = "black",
                    fill = "white",
                    alpha = 0.8) +
  theme_sankey(base_size = 14) +
  labs(title = "Site Attrition Through Filtering Process",
       subtitle = "Progressive reduction and augmentation of monitoring sites") +
  theme(axis.title = element_blank(),
        axis.text.x = element_text(hjust = 0.5,  # Center alignment
                                   face = "bold",
                                   margin = margin(t = 10),  # Add top margin
                                   lineheight = 0.8),  # Tighter line height for multi-line text
        panel.grid = element_blank(),
        plot.title = element_text(face = "bold", size = 16),
        plot.subtitle = element_text(color = "gray40"),
        plot.background = element_rect(fill = "white", color = NA))
```

This diagram illustrates the progressive filtering and augmentation of the
gages through our filtering process with a final site count of `r nrow(filtered_transbasin_watersheds)`.
```{r, echo=FALSE}
sankey_plot
```

The following sections detail each step of this selection process, including the 
specific methods, data sources, and quality control measures employed.

# Initial NWIS gage data pull

## First, we grab all USGS stream gages for states of interest:
- Identified and retrieved all USGS stream gages that measure discharge (parameter 
code "00060") within our four states of interest (Colorado, Wyoming, Utah, and 
Kansas) using the NWIS (National Water Information System) database. 
- Filtered the gages to include only those with watersheds smaller than 1500 km<sup>2</sup>.
- Processed the data in to a spatial format.

### Initial data compilation:
_This filtered dataset comprises 2,276 sites spanning all states of interest._

```{r, eval=FALSE}
states_oi <- c("Colorado", "Wyoming", "Utah", "Kansas", "New Mexico")

us_sf_object <- tigris::states() %>% 
  filter(NAME %in% states_oi) 

# Get a list of NWIS sites for all of the states
nwis_sites_by_state <- map(us_sf_object$STUSPS, 
                           ~{
                             discharge_sites <- whatNWISsites(stateCd = .x, parameterCd = "00060") %>% 
                               filter(site_tp_cd == 'ST') 
                             
                             # Only use gages under 1500 square kilometers (as defined by the USGS):
                             filtered_nwis_call <- readNWISsite(discharge_sites$site_no) %>% 
                               mutate(drain_area_km = drain_area_va *  2.58999) %>%
                               filter(drain_area_km <= 1500) %>% 
                               # For future tracking with the NLDI:
                               mutate(nldi_compatible_site_id = paste0("USGS-",site_no),
                                      STUSPS = .x)

                             return(filtered_nwis_call)
                           }
) 

nwis_sites <- bind_rows(nwis_sites_by_state) %>%
  st_as_sf(coords = c("dec_long_va", "dec_lat_va"), crs = 4269)
```

```{r, echo=FALSE, warning=FALSE}
# write_rds(nwis_sites, here("data", "nwis_sites_pull.rds"))

nwis_sites <- read_rds(here("data", "nwis_sites_pull.rds")) %>% 
  arrange(STUSPS) 

datatable(nwis_sites,
          options = list(
            scrollX = TRUE,
            pageLength = 5
          ))
```

```{r, echo=FALSE, warning=FALSE}
mapview(nwis_sites, zcol = "STUSPS", layer.name = "State")
```

# Filter gages based on length of time series

## Next, we will filter the USGS sites that we retrieved based on the following criteria:
- Filter for "Daily" data type
- Measurements started by 1980 and continued measuring through 2010
- Require at least 30 years of observations

### Filter based on data availability:
_The next filtering process reduces the initial dataset from `r nrow(nwis_sites)` to `r nrow(data_filter_nwis_sites)` sites._

```{r, eval = FALSE}
# First, we retrieve the complete metadata for all identified gages
gage_meta <- dataRetrieval::whatNWISdata(siteNumber = nwis_sites$site_no, parameterCd = "00060")

# We then obtain and process reference information from USGS to help interpret the metadata
# by scraping parameter code definitions and site type classifications from USGS web pages
# and joining this information with our gage metadata

# Scrape the USGS water parameter code table 
tables <- rvest::read_html('https://help.waterdata.usgs.gov/parameter_cd?group_cd=%') %>% # fetch the webpage
  rvest::html_nodes('table') %>% # extract the table elements from the HTML
  rvest::html_table() # convert the HTML tables into R data frames

# create parm_cd column in the USGS water parameter code table
pcodes <- tables[[1]] %>% 
  janitor::clean_names() %>% 
  dplyr::mutate(parm_cd = stringr::str_pad(as.character(parameter_code), 5, pad = "0"))

inventory <- gage_meta %>%
  dplyr::left_join(pcodes, by = "parm_cd") %>%
  dplyr::select(c(site_name = station_nm,
                  site_no,
                  data_type_cd,
                  site_type_cd = site_tp_cd,
                  n_obs = count_nu,
                  begin_date,
                  end_date,
                  parameter = parameter_name_description,
                  code = parm_cd))

# Scrape the USGS NWIS Site types tables 
table <- rvest::read_html('https://maps.waterdata.usgs.gov/mapper/help/sitetype.html') %>%
  rvest::html_nodes('table') %>%
  rvest::html_table() 

table <- rbind(table[[1]],table[[2]],table[[3]],table[[4]],table[[5]]) %>%
  dplyr::select(site_type_cd = 1,
                site_type = 2)

inventory <- left_join(inventory, table, by = 'site_type_cd') %>%
  mutate(data_type = case_when(data_type_cd == "dv" ~ "Daily",
                               data_type_cd == "uv" ~ "Unit",
                               data_type_cd == "qw" ~ "Water Quality",
                               data_type_cd == "gw" ~ "Groundwater Levels",
                               data_type_cd == "iv" ~ "Unit",
                               data_type_cd == "sv" ~ "Site Visits",
                               data_type_cd == "pk" ~ "Peak Measurements",
                               data_type_cd == "ad" ~ "USGS Annual Water Data Report",
                               data_type_cd == "aw" ~ "Active Groundwater Level Network",
                               data_type_cd == "id" ~ "Historic Instantaneous"))

# The filtering step selects only gages with:
new_data_gages <- inventory %>%
  filter(data_type == "Daily", # Daily flow measurements
         year(begin_date) <= 1980, # Long-term record starting by 1980
         year(end_date) >= 2010, # Continuing at least until 2010
         n_obs >= 365*30) # Minimum of 30 years of daily observations
        

# Finally, we update our spatial gage dataset to only include these high-quality sites
data_filter_nwis_sites <- nwis_sites %>% 
  filter(site_no %in% new_data_gages$site_no)
```

```{r, echo=FALSE, warning=FALSE}
# write_rds(data_filter_nwis_sites, here("data", "initial_filter_nwis_sites.rds"))

data_filter_nwis_sites <- read_rds(here("data", "initial_filter_nwis_sites.rds")) %>% 
  arrange(STUSPS)

datatable(data_filter_nwis_sites,
          options = list(
            scrollX = TRUE,
            pageLength = 5
          ))
```

```{r}
mapview(data_filter_nwis_sites, zcol = "STUSPS", layer.name = "State")
```

# Grab CO DWR gages

## Next, we will grab the Colorado Division of Water Resources (CDWR) gages based on the same criteria as the USGS gages:
- Expanded stream gage dataset via the CDWR API
- We require the same QC for this data: records must begin by 1980, continue 
until at least 2010, and have a minimum of 30 years of data.
- Other requirements for this data: filtered specifically for stream gages which 
had geographic coordinates and USGS identifiers.

### Add CDWR sites to USGS sites:
_This data extraction results in `r nrow(cdwr_sites)` new CDWR sites, and an updated overall site yield of `r nrow(gage_sites)`._
_Some of the added sites are in New Mexico, and are tracked in CDWR's system. These sites get filtered out in later steps._

```{r, eval = FALSE}
# Access Colorado's Division of Water Resources (CDWR) stream gage data via their REST API
cdwr_sites <- httr::GET(url = "https://dwr.state.co.us/Rest/GET/api/v2/surfacewater/surfacewaterstations/?format=json&fields=stationNum%2Cabbrev%2CusgsSiteId%2CstationName%2CutmX%2CutmY%2Clatitude%2Clongitude%2CstartDate%2CendDate%2CmeasUnit") %>%
  httr::content(., as = "text", encoding = "UTF-8", show_col_types = FALSE) %>%
  jsonlite::fromJSON() %>%
  .[["ResultList"]] %>% 
  # Apply those same filters that were applied to the USGS sites.
  filter(year(startDate) <= 1980, # Records must begin by 1980
         year(endDate) >= 2010, # Records must extend at least to 2010
         !is.na(usgsSiteId), # Site must have a USGS ID
         !is.na(longitude) & !is.na(latitude), # Site must have valid coordinates
         !(usgsSiteId %in% nwis_sites$site_no), # Prevent duplicating gages already in our USGS dataset
         abbrev %in% c(read_csv("data/cdwr.csv", show_col_types = FALSE) %>%.$Abbrev)) %>% # Filter for stream gage types
  st_as_sf(coords = c("longitude", "latitude"), crs = 4269) %>% 
  # Standardize column names to match our USGS dataset structure
  select(site_no = usgsSiteId,
         station_nm = stationName) %>% 
  # Add additional metadata fields needed for consistency with USGS data
  mutate(nldi_compatible_site_id = paste0("USGS-", site_no),
         agency_cd = "CDWR",
         STUSPS = "CO") 

# Combine CDWR sites with our USGS dataset to create a comprehensive gage network
gage_sites <- data_filter_nwis_sites %>% bind_rows(cdwr_sites)
```

```{r, echo=FALSE, warning=FALSE}
# write_rds(cdwr_sites, here("data", "cdwr_sites.RDS"))

cdwr_sites <- read_rds(here("data", "cdwr_sites.RDS"))

# write_rds(gage_sites, here("data", "usgs_cdwr_sites.RDS"))

gage_sites <- read_rds(here("data", "usgs_cdwr_sites.RDS")) %>% 
  arrange(STUSPS)

datatable(gage_sites,
          options = list(
            scrollX = TRUE,
            pageLength = 5
          ))
```

```{r, echo=FALSE, warning=FALSE}
mapview(gage_sites, zcol = "STUSPS", layer = "State")
```


# Delineate stream gage watersheds

## Next, we will delineate the watersheds that feed into the selected gages:
- Linking stream gages to NHDPlus Flowlines: Established the connection between
each stream gage and its corresponding NHDPlus flowline using two methods
  - NLDI (Network Linked Data Index) service which provides USGS-verified COMIDs
  - Coordinate based approach that identifies the nearest NHDPlus flowline based 
  on the gage's geographic location
- Quality control for flowline selection: To maintain high-quality watershed delineations, 
we retained only gages where both methods yielded the same COMID. This ensured that
spatial mismatches were not included in the final dataset. 
- Watershed delineation algorithm: We developed a watershed delineation function 
that leverages the NHDPlus network topology to "walk" upstream from each gage's 
flowline to identify all contributing stream segments and their associated 
catchments. These catchments are then dissolved into a single watershed polygon 
representing the complete upstream contributing area. 

### Delineated stream gage updates to the site yield:
_Sites are dropped during the COMID retrieval and filtering step_
_Sites are dropped during the watershed delineation step ([504 errors])._
_Sites are dropped during the final area filtering step_
_Ultimately, we are left with `r nrow(watershed_polygons)` sites after the delineation steps._

```{r, eval=FALSE}
# Identifying stream network locations for each gage
## This section associates each gage with its correct location in the NHDPlus stream network
gage_sites_COMID <- gage_sites %>%
  rowwise() %>%
  mutate(
    # Method 1: Use NLDI service to get the "official" COMID based on the gage ID
    comid = possibly(
      function(x) {
        result <- try(get_nldi_feature(list(featureSource = "nwissite", featureID = x))$comid, silent = TRUE)
        if (inherits(result, "try-error") | is.null(result)) NA_character_ else result
      },
      otherwise = NA_character_,
      quiet = TRUE)(nldi_compatible_site_id),
    # Method 2: Use the gage's coordinates to find the closest NHDPlus flowline
    comid_coords = {
      result <- try(discover_nhdplus_id(geometry), silent = TRUE)
      if (inherits(result, "try-error")) NA else result}) %>% 
  ungroup() %>%
  # QC to ensure accurate watershed delineation
  mutate(comid_new = ifelse(is.na(comid), comid_coords, comid)) %>%
  filter(comid_coords == comid) %>% # Only keep gages where both methods identify the same stream segment (COMID)
  select(STUSPS, site_no, station_nm, comid = comid_new) %>%
  mutate(comid = as.numeric(comid)) 
  
# Watershed delineation process
## Load the NHDPlus flow network table, which contains the topological 
## relationships between all stream segments in the Continental US
nhd <- read_csv(here("data", "nhd_flow_network.csv"), show_col_types = FALSE)

# Define `watershed_delineator()` function that performs watershed delineation for a single gage
watershed_delineator <- function(STUSPS, site_no, station_nm, comid, geometry) {
  
  # 1. Retrieve upstream COMIDs ----
  upstream <- tryCatch(
    suppressWarnings(nhdplusTools::get_UT(nhd, comid)),
    error = function(e) {
      message("Error retrieving upstream COMIDs for ", site_no, ": ", e$message)
      return(NULL)
    }
  )
  
  ## Handle cases where no upstream COMIDs are found (headwater site)
  if (is.null(upstream) || length(upstream) == 0) {
    message("No upstream COMIDs found for ", site_no, " - treating as headwater site")
    upstream <- comid  # Use the site's own COMID
  }
  
  # 2. Retrieve catchment polygons for all upstream segments ----
  catchments <- possibly(
    nhdplusTools::get_nhdplus,
    otherwise = NULL,
    quiet = TRUE
  )(
    comid = upstream,
    realization = "catchment",
    t_srs = 4269
  )
  
  # 3. Process catchment data into a single watershed polygon ----
  if (is.null(catchments)) {
    message("Catchment retrieval failed for ", site_no)
    return(NULL)
  }
  
  watershed <- catchments %>%
    st_make_valid() %>% # Ensure geometries are valid
    distinct(featureid, .keep_all = TRUE) %>% # Remove duplicates
    summarize() %>%  # Dissolve into single polygon
    nngeo::st_remove_holes() %>% # Remove holes
    # Add identifying metadata
    mutate(
      STUSPS = STUSPS,
      site_no = site_no,
      comid = comid
    ) %>%
    st_as_sf() # convert to sf object if not already
  
  message("Successfully processed: ", station_nm, "\n")
  Sys.sleep(2)
  return(watershed)
}

# Perform watershed delineation for all gages
watershed_polygons <- pmap(gage_sites_COMID, safely(watershed_delineator), .progress = TRUE) %>% 
  transpose() %>%  # Separate results and errors
  pluck(1) %>%  # Extract results
  compact() %>% # Remove NULL results
  bind_rows() %>% # Combine into a single data frame
  mutate(area = sf::st_area(.)) %>% 
  filter(as.numeric(area) <= 1.6e+9) # Remove any watersheds larger than 1600km^2 
```

```{r, echo=FALSE, warning=FALSE}

# write_rds(watershed_polygons, here("data", "watershed_polygons.RDS"))

watershed_polygons <- read_rds(here("data", "watershed_polygons.RDS")) 

datatable(watershed_polygons,
          options = list(
            scrollX = TRUE,
            pageLength = 5
          ))
```

```{r, echo=FALSE, warning=FALSE}
mapview(watershed_polygons, zcol = "STUSPS", layer.name = "State")
```

# Grab filtering variables from StreamCAT 

## Next, we retrieve anthropogenic watershed characteristics from EPA's StreamCat database.
- StreamCAT provides standardized watershed-level summaries of natural and 
anthropogenic landscape features for streams across the conterminous United States.
- We retrieved those variables that help filter the data. During the predictor
variable acquisition process we pick up more variables from StreamCAT.
  - Urban land cover (high, medium, low density, and open urban areas)
  - Artificial modifications (canal density, dam density, dam storage)
- We filtered the watersheds for:
  - Less than 10% urban development
  - Dam storage density below 100,000 ML/km² threshold

### Filter watersheds based on StreamCAT dataset
_The resulting data set contains only watersheds with minimal human modification, suitable for establishing reference conditions_
_`r nrow(streamcat_watersheds)` watersheds remain after the StreamCAT filtering step._
```{r, eval=FALSE}
# Urban cover in 2019 (from NLCD 2019)
urban_cover <- c(
  "pcturbop2019",  # Percentage of open urban land cover in 2019 (parks, golf courses, large-lot development)
  "pcturbmd2019",  # Percentage of medium density urban land cover in 2019 (mix of constructed materials and vegetation)
  "pcturblo2019",  # Percentage of low density urban land cover in 2019 (single-family housing areas)
  "pcturbhi2019"   # Percentage of high density urban land cover in 2019 (apartments, commercial/industrial)
)

# Artificial modifications (from NHD and National Inventory of Dams)
artificial_modifications <- c(
  "canaldens",   # Canal density (km/km²) 
  "damdens",     # Dam density (dams/km²)
  "damnidstor",  # Dam normal storage (ML/km²)
  "damnrmstor"   # Dam maximum storage (ML/km²)
)

# Retrieve filtering watershed attributes from the EPA's StreamCat database
streamcat_vars <- StreamCatTools::sc_get_data(
  # Combine all our desired metrics into a single comma-separated string
  metric = paste(c(urban_cover, artificial_modifications), collapse = ","),
  aoi = 'watershed',  # Query for entire watershed upstream of each point
  comid = watershed_polygons$comid) %>%  # Use our previously identified COMIDs
  # remove excess variables:
  select(-contains("AREASQKM"))

# Combine the watershed polygons with the streamcat data
streamcat_watersheds <- watershed_polygons %>%
  left_join(., streamcat_vars, by = "comid") %>%
  # Calculate total urban land cover by summing all urban types from NLCD 2019
  # (high intensity + medium intensity + low intensity + open urban)
  mutate(pcturb2019ws = pcturbhi2019ws + pcturbmd2019ws + pcturblo2019ws + pcturbop2019ws) %>%
  filter(pcturb2019ws < 10,     # Less than 10% urban development
         damnidstorws < 100000) # Dam storage density below 100,000 ML/km² threshold
```

```{r, echo=FALSE, warning=FALSE}
# write_rds(streamcat_watersheds, here("data", "streamcat_watersheds.RDS"))

streamcat_watersheds <- read_rds(here("data", "streamcat_watersheds.RDS"))

datatable(streamcat_watersheds,
          options = list(
            scrollX = TRUE,
            pageLength = 5
          ))
```

```{r, echo=FALSE, warning=FALSE}
mapview(streamcat_watersheds, zcol = "STUSPS", layer.name = "State")
```

# Filter watersheds based on flowline intersections

## Finally, we examine watershed boundaries for artificial water transfers.
- Transbasin diversions are artificial water transfers where water is moved between
natural drainage basins.
- The detection algorithm for transbasin diversions is as follows:
  - Fetch high resolution NHD flowlines for each watershed using the 
  NHDPlus High Resolution (NHDPlus HR) service provided by the USGS.
  - Classified flowlines as either 'natural' or 'unnatural' types.
  - Detected locations where unnatural flowlines cross watershed boundaries,
  indicating potential water transfers.
  - Created visual confirmation maps for each watershed, highlight problematic 
  crossings in red.

### The first step in the transbasin diversion filter is finding the flowlines for each watershed.
These are some taxing functions, so we filter as much as possible before this step to 
minimize the amount of watersheds that we need to find the flowlines for. 

```{r, eval=FALSE}
# Open a connection to the National Hydrography Dataset High Resolution (NHD HR)
# web service, which provides detailed stream network data for the entire
# United States
nhd_hr <- arcgislayers::arc_open("https://hydro.nationalmap.gov/arcgis/rest/services/NHDPlus_HR/MapServer")
nhd_hr_flowlines <- arcgislayers::get_layer(nhd_hr, 3) # Layer 3 contains flowlines

# Retry wrapper for `arcgislayers::arc_select()` ----
# This wrapper will attempt to query the NHD HR service multiple times if it 
# encounters errors (often due to excess concurrent server requests)
retry_arc_select <- function(nhd_flowlines, filter_geom_arg, crs_arg, site_info = "", max_attempts = 5) {
  for (attempt in 1:max_attempts) {
    # Try to query NHD HR, returning NULL if it fails
    result <- tryCatch({
      message("Attempt ", attempt, " for ", site_info)
      arcgislayers::arc_select(
        nhd_flowlines, 
        filter_geom = filter_geom_arg,
        crs = crs_arg)
    }, error = function(e) {
      message("\nAttempt ", attempt, " failed for ", site_info, ": ", e$message, "\n")
      NULL
    })
    # If successful, return the result immediately
    if (!is.null(result)) return(result)
    # If unsuccessful, wait before trying again with exponential backoff
    wait_time <- 2^attempt + runif(1, 0, 1)
    message("\nWaiting ", round(wait_time, 1), " seconds before retry...")
    Sys.sleep(wait_time)  # Wait before retrying
  }
  # If all attempts fail, return NULL
  message("\nAll attempts failed for ", site_info)
  return(NULL)
}

# `fetch_flowlines()` function ----
# This function retrieves all flowlines within and around a watershed, then 
# classifies them by type
fetch_flowlines <- function(site_no, STUSPS, geometry, nhd_flowlines = nhd_hr_flowlines){ 
  
  # Identifier for logging purposes
  site_info <- paste0(site_no, " (", STUSPS, ")")
  message("\nProcessing site: ", site_info)
  
  # Buffer the watershed to ensure we capture all relevant flowlines
  watershed_aoi <- tryCatch({
    # Ensure geometry is handled properly
    if(inherits(geometry, "sfc")) {
      geom_with_crs <- geometry
    } else {
      geom_with_crs <- st_sfc(geometry, crs = 4269) # NAD83 coordinate system
    }
    st_buffer(geom_with_crs, 1000) # 1km buffer
  }, error = function(e) {
    message("Couldn't create watershed AOI for ", site_info)
    return(NULL)
  })
  
  # Exit early if buffer fails
  if (is.null(watershed_aoi)) {
    message("Couldn't create watershed AOI for ", site_info)
    return(NULL)
  }
  
  # Query NHD HR using retry wrapper for `arc_select()`
  arc_search_result <- retry_arc_select(
    nhd_flowlines, 
    filter_geom_arg = watershed_aoi, 
    crs_arg = st_crs(watershed_aoi),
    site_info = site_info
    )
  
  # Exit early if no flowlines are found
  if (is.null(arc_search_result) || nrow(arc_search_result) == 0) {
      message("\nNo flowlines found for site ", site_info)
      return(NULL)
    }
  
  # Process and classify flowlines by their type
  flowlines <- tryCatch({
    arc_search_result %>%
      st_make_valid() %>%
      dplyr::distinct() %>%
      mutate(
        # Classify each flowline based on its feature type code (ftype)
        flowline_type = case_when(
          ftype == 460 ~ "natural",             # Natural streams/rivers
          ftype == 558 ~ "artificial path",     # Artificial flowpaths through waterbodies
          ftype == 468 ~ "drainageway",         # Constructed drainage features
          ftype == 336 ~ "canal ditch",         # Human-made canals and ditches
          ftype == 566 ~ "coastline",           # Coastline features
          ftype == 334 ~ "connector",           # Artificial connectors in network
          ftype == 428 ~ "pipeline",            # Underground or above-ground pipes
          ftype == 420 ~ "underground conduit", # Subsurface flow paths
          .default = "unnatural"                # Any other types default to unnatural
        ),
        # Add metadata for tracking
        site_no = site_no,
        STUSPS = STUSPS
      )
  }, error = function(e) {
    message("\nError processing flowlines for ", site_info, ":", e$message)
    return(NULL)
  })
  
  return(flowlines)
}

# Set up parallel processing ----

# Set up parallel processing with fewer workers to reduce server load
num_workers <- min(availableCores() - 1, 3) # Use at most 3 workers
message("Setting up parallel processing with ", num_workers, " workers")
plan(multisession, workers = num_workers)

# Explicitly export the nhd_hr_flowlines object to workers
furrr_options(
  globals = TRUE,
  packages = c("dplyr", "sf", "arcgislayers")
)

# Select the data we need 
site_data <- streamcat_watersheds %>% 
  select(site_no, STUSPS, geometry)

# Process in chunks to reduce memory pressure and server load
chunk_size <- 10
total_sites <- nrow(site_data)
chunks <- split(1:total_sites, ceiling(seq_along(1:total_sites) / chunk_size))

all_results <- list()
all_successful <- list()

for (chunk_idx in seq_along(chunks)) {
  
  message("\n=== Processing chunk ", chunk_idx, " of ", length(chunks), " ===")
  
  # Get the indices for this chunk
  indices <- chunks[[chunk_idx]]
  chunk_data <-site_data[indices, ]
  
  # Process this chunk in parallel
  chunk_results <- future_pmap(
    list(
      site_no = chunk_data$site_no,
      STUSPS = chunk_data$STUSPS,
      geometry = chunk_data$geometry
    ),
    safely(function(site_no, STUSPS, geometry){
      fetch_flowlines(site_no, STUSPS, geometry, nhd_flowlines = nhd_hr_flowlines)
    }),
    .progress = TRUE
  )
  
  # Progress chunk results
  chunk_transposed <- transpose(chunk_results)
  chunk_successful <- chunk_transposed %>% pluck(1) %>% compact()
  chunk_errors <- chunk_transposed %>% pluck(2) %>% compact()
  
  # Print diagnostics for this chunk
  message("Chunk ", chunk_idx, ": ", length(chunk_successful), " successful, ", 
          length(chunk_errors), " errors")
  
  # Add successful results to our collection
  all_successful <- c(all_successful, chunk_successful)
  all_results <- c(all_results, chunk_results)
  
  # Take a short break between chunks to avoid overloading the server with parallel requests
  if (chunk_idx < length(chunks)) {
      message("Taking a short break before next chunk...")
      Sys.sleep(5)
    }
    
}

# Process final results ----
results_transposed <- transpose(all_results)
successful_results <- results_transposed %>% pluck(1) %>% compact()
errors <- results_transposed %>% pluck(2) %>% compact()

# Make and save combined flowlines
if (length(successful_results) > 0) {
  message("Combining and saving all flowlines...")
  all_flowlines <- bind_rows(successful_results)
  
  # Save combined flowlines
  write_rds(all_flowlines, here("data", "all_flowlines.RDS"))
  
  message("All done! Processed ", length(successful_results), " sites with a total of ", 
          nrow(all_flowlines), " flowlines.")
} else {
  message("No successful results to process.")
}

# Save individual flowlines
walk(successful_results, function(df) {
  site <- unique(df$site_no)
  state <- unique(df$STUSPS)
  write_rds(df, here("data", "flowlines2", paste0(state, "_", site, ".RDS"))) # flowlines2 for testing purposes
})
```

###An example of a watershed with no transbasin diversions

![Example of a watershed with no transbasin diversions](/Users/juandlt_csu/Library/CloudStorage/OneDrive-Colostate/flow_prediction/data/transbasin_confirm/NATURAL_CO_07105000_.png)
###An example of a watershed with transbasin diversions

![An example of a watershed with transbasin diversions](/Users/juandlt_csu/Library/CloudStorage/OneDrive-Colostate/flow_prediction/data/transbasin_confirm/NATURAL_CO_09165000_.png )

```{r, eval=FALSE}
# Transbasin Diversion Detection System ----
# This section identifies watersheds where artificial water transfers cross 
# natural drainage boundaries. These diversions compromise the water balance 
# and are excluded from reference watersheds

# `transbasin_finder()` function ----
# This function detects transbasin diversions for a single watershed based on
# the artificial flowlines crossing the watershed boundary. 

transbasin_finder <- function(site_no, site_data = streamcat_watersheds) {

  # Filter our master list to just the gage watershed we are iterating over 
  site <- site_data %>% 
    filter(site_no == !!site_no)
  
  # Exit early if site doesn't exist in our dataset
  if (nrow(site) == 0) {
    message("\nSite ", site_no, " not found in reference data")
    return(NULL)
  }
  
  # Identifier for logging purposes
  site_info <- paste0(site_no, " (", site$STUSPS, ")")
  message("\nProcessing ", site_info)
  
  # Try to read the flowline file for this watershed
  # flowlines_path <- here("data", "flowlines2", paste0(site$STUSPS, "_", site$site_no, ".RDS"))
  
  # Check if file exists, and exit early if it doesn't
  if(!file.exists(flowlines_path)) {
    message("\nFlowline file not found for ", site_info,)
    return(NULL)
  }
  
  # Read the flowline data
  flowlines <- tryCatch({
    read_rds(flowlines_path)
  }, error = function(e) {
    message("Error reading flowlines for ", site_info, ": ", e$message)
    return(NULL)
  })
  
  # Exit early if no flowlines are found in the file
  if (is.null(flowlines) || (is.data.frame(flowlines) & nrow(flowlines) == 0)) {
    message(site_info, " has no flowlines data")
    return(NULL)
  }
  
  # Filter for unnatural flowlines in and around the watershed
  flowlines_unnatural <- tryCatch({
    flowlines %>% 
      filter(flowline_type != "natural")
  }, error = function(e) {
    message("Error filtering unnatural flowlines for ", site_info, ": ", e$message)
    return(NULL)
  })
  
  # Detect if artificial flowlines cross the watershed boundary
  watershed_processed <- tryCatch({
    # For linestring transformation to work, watershed must be a polygon
    site_geom <- site
    
    # Convert watershed to polygon if needed
    if (st_geometry_type(site_geom)[1] != "POLYGON") { 
      site_geom <- st_cast(site_geom, "POLYGON")
    }
    
    # Create polyline from the watershed boundary
    polyline <- site_geom %>% st_cast("LINESTRING")
    
    # >>> Check if unnatural flowlines intersect the watershed boundary <<<
    crossovers <- 0
    if (nrow(flowlines_unnatural) > 0) {
      # Spatial intersection to find crossovers
      crossovers <- flowlines_unnatural %>%
        st_intersection(polyline) %>%
        nrow() # Count the number of crossings
    }
    
    # Classify the watershed based on boundary crossings
    site_geom %>% 
      group_by(site_no, comid) %>% 
      summarize(.groups = "drop") %>%
      mutate(transbasin = ifelse(crossovers > 0, "TRANSBASIN_DIVERSION", "NATURAL"))
    
  }, error = function(e) {
    message("Error in spatial analysis for ", site_info, ": ", e$message)
    return(NULL)
  })
  
  # Exit early if spatial analysis for unnatural intersections failed
  if (is.null(watershed_processed)) {
    return(NULL)
  }
  
  # Create visualization of watershed and flowlines for verification 
  tryCatch({
    # Extract the bounding box
    bbox_site <- st_bbox(watershed_processed)
    
    # Create the ggplot map
    gg_map <- ggplot() +
      # Plot the watershed
      geom_sf(data = watershed_processed, color = "black", fill = "white", size = 1) + 
      # Plot the site point (with safe filtering)
      {
        site_point <- nwis_sites %>% filter(site_no == site$site_no)
        if (nrow(site_point) > 0) {
          geom_sf(data = site_point, color = "lightblue", size = 5.5)
        }
      } +
      # Plot all flowlines in blue
      geom_sf(data = flowlines, color = "blue", size = 0.5) + 
      # Plot unnatural flowlines in red (if they exist)
      {
        if (nrow(flowlines_unnatural) > 0) {
          geom_sf(data = flowlines_unnatural, color = "red", size = 2)
        }
      } +
      # Set map extents
      xlim(bbox_site["xmin"], bbox_site["xmax"]) +
      ylim(bbox_site["ymin"], bbox_site["ymax"]) +
      coord_sf() + 
      theme_void() +
      labs(title = paste0(site$site_no, " ", watershed_processed$transbasin[1])) +
      theme(
        plot.title = element_text(size = 14, hjust = 0.5),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()
      )
    
    # Create output directory if it doesn't exist
    # dir.create(here("data", "transbasin_confirm2"), showWarnings = FALSE, recursive = TRUE) # transbasin confirm2 for testing purposes
    
    # Save the map as an image
    output_file <- here("data", "transbasin_confirm2", 
                        paste0(watershed_processed$transbasin[1], "_", 
                               site$STUSPS, "_", site$site_no, ".png"))
    
    ggsave(output_file, plot = gg_map, width = 8, height = 6, dpi = 100)
    message("Successfully created visualization for ", site_info)
    
  }, error = function(e) {
    message("Error creating visualization for ", site_info, ": ", e$message)
    # Return watershed data even if visualization fails
  })
  
  message("Completed processing for ", site_info)
  return(watershed_processed)
  
}

# Set up parallel processing ----

# Set up parallel processing with fewer workers to reduce server load
num_workers <- min(availableCores() - 1, 8) # Use at most 8 workers
message("Setting up parallel processing with ", num_workers, " workers")
plan(multisession, workers = num_workers)

# Explicitly export the nhd_hr_flowlines object to workers
furrr_options(
  globals = TRUE,
  packages = c("dplyr", "sf", "ggplot2", "readr", "here")
)

# Chunk the data and run it through `transbasin_finder()` ----

# Get site numbers to process
site_numbers <- streamcat_watersheds$site_no
total_sites <- length(site_numbers)

# Process in chunks to reduce memory pressure
chunk_size <- 20
total_sites <- nrow(site_numbers)
chunks <- split(site_numbers, ceiling(seq_along(site_numbers) / chunk_size))

# Process each chunk
transbasin_finder_results <- list()

for (chunk_idx in seq_along(chunks)) {
  message("=== Processing chunk ", chunk_idx, " of ", length(chunks))
  
  # Get the sites for this chunk
  chunk_sites <- chunks[[chunk_idx]]
  
  # Process this chunk in parallel
  chunk_results <- future_map(
    chunk_sites,
    safely(function(site_no){
      transbasin_finder(site_no)
    }),
    .progress = TRUE
  )
  
  # Extract successful results
  chunk_successful <- chunk_results %>% 
    transpose() %>% 
    pluck(1) %>% 
    compact()
  
  message("Chunk ", chunk_idx, ": Processed ", length(chunk_successful), 
            " sites successfully out of ", length(chunk_sites))
  
  # Add successful results to our collection
  transbasin_finder_results <- c(transbasin_finder_results, chunk_successful)
  
  # Pause between chunks
  if (chunk_idx < length(chunks)) {
      message("Taking a short break before the next chunk...")
      Sys.sleep(2)
    }
}

# Combine the final results ----
if (length(transbasin_finder_results) > 0) {
  
  watersheds_div <- transbasin_finder_results %>% 
    compact() %>% 
    bind_rows() %>% 
    st_make_valid()
  
  # Save the final results
  write_rds(watersheds_div, here("data", "watersheds_div.RDS"))

}
```

```{r, eval=FALSE}
# Reduce our gages to only gages without a transbasin diversion:
filtered_transbasin_watersheds <- watersheds_div %>%
  filter(transbasin == "NATURAL")
```

```{r, echo=FALSE, warning=FALSE}
# write_rds(watersheds_div, here("data", "watersheds_div.RDS"))

watersheds_div <- read_rds(here('data', 'watersheds_div.rds'))

filtered_transbasin_watersheds <- watersheds_div %>% 
  filter(transbasin == "NATURAL")

datatable(filtered_transbasin_watersheds,
          options = list(
            scrollX = TRUE,
            pageLength = 5
          ))
```

```{r map, echo=FALSE, warning=FALSE}
mapview(filtered_transbasin_watersheds, label = "Natural Watersheds")
```
